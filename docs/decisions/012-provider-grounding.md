# 012: Provider-Level Grounding via `grounding` on LLMStep

**Status:** Accepted
**Date:** 2026-02-21

**Context:** Lattice supports web search via a two-step pattern (`FunctionStep` + `web_search()` factory wrapping OpenAI Responses API), but this is OpenAI-only and requires two pipeline steps for what is conceptually one operation. All three major providers now offer native web search as a server-side tool: OpenAI (`web_search` in Responses API), Anthropic (`web_search_20250305` server tool), and Google (`google_search` grounding tool). Supporting these natively in `LLMStep` enables single-step grounded enrichment with provider-agnostic configuration. The pre-launch timing also makes this the right moment to rewrite `OpenAIClient` from Chat Completions to the Responses API (OpenAI's recommended path), gaining native tool support, inline citations, and the `text.format` structured output surface.

**Decision:** Add `grounding: bool | dict | GroundingConfig | None` to `LLMStep`. `True` enables with defaults, a dict or `GroundingConfig` (Pydantic model, `extra="forbid"`) provides fine-grained control (`allowed_domains`, `blocked_domains`, `user_location`, `max_searches`). LLMStep normalises the parameter and builds a generic `{"type": "web_search", ...}` tools dict, passed to the LLMClient via a new optional `tools` parameter on `complete()`. Each provider adapter translates to its native format: OpenAI passes `tools=[{"type": "web_search", ...}]` to the Responses API; Anthropic translates to `web_search_20250305` server tool with `allowed_domains`, `blocked_domains`, `user_location`, `max_uses`; Google translates to `types.Tool(google_search=types.GoogleSearch())` with `exclude_domains` (mapped from `blocked_domains`). Citations are normalised to `Citation(url, title, snippet)` on `LLMResponse.citations`, and LLMStep injects them as `__sources` (internal field, filtered from output, available in `prior_results`). The cache key includes a hash of the grounding config. Structured outputs (`json_schema`) are disabled when grounding tools are active for Anthropic and Google (incompatible with their citation systems), falling back to `json_object` prompting. OpenAI Responses API supports both simultaneously via `text.format`. The `OpenAIClient` is rewritten to use the Responses API for native OpenAI (no `base_url`) and falls back to Chat Completions for `base_url` providers (Ollama, Groq, etc.) for maximum compatibility. Custom clients that don't accept the `tools` parameter get a clear `StepError` (TypeError caught) when grounding is enabled.

**Alternatives considered:** (1) **Separate GroundedLLMStep class** — rejected because it duplicates the entire LLMStep interface for a single parameter toggle; composition via a parameter is simpler. (2) **`web_search` naming instead of `grounding`** — rejected because "grounding" is more generic and future-proof (could encompass RAG, knowledge bases, not just web search). (3) **Keep Chat Completions for OpenAI** — rejected because Responses API is OpenAI's recommended path, provides native tool support and inline citations, and the pre-launch timing means no breaking change. (4) **LiteLLM for multi-provider tools** — rejected per existing design principle of minimal dependencies and direct provider control. (5) **Always enable structured outputs with grounding** — rejected because Anthropic's `output_config.format` and Google's `response_json_schema` are incompatible with their respective search tools; the `json_object` fallback via prompting works reliably across all providers.

---

**Update (2026-02-22): `provider_kwargs` escape hatch.** Added `provider_kwargs: Optional[Dict[str, Any]]` to `GroundingConfig`. Keeps `extra="forbid"` for typo protection on the 4 cross-provider fields, while providing a pass-through for provider-specific options (e.g. OpenAI `search_context_size`, Google `dynamic_retrieval_config`). Each adapter merges `provider_kwargs` into its native tool configuration after Lattice's own field mappings. Cache key automatically includes `provider_kwargs` via `model_dump_json()`.
