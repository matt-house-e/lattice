{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Accrue v0.4 Demo\n",
    "\n",
    "This notebook demonstrates the Accrue pipeline across **Phases 2, 3, and 4**.\n",
    "\n",
    "**Phase 2 — Resilience & API redesign:**\n",
    "- `Pipeline.run(df)` returns `PipelineResult` with `.data`, `.cost`, `.errors`, `.success_rate`\n",
    "- Per-step cost aggregation (`CostSummary` with token counts per step)\n",
    "- Per-row error handling (`on_error=\"continue\"` / `\"raise\"`, `RowError`)\n",
    "- `EnrichmentConfig` presets (`for_development()`, `for_production()`, `for_server()`)\n",
    "- Provider flexibility via `base_url` shortcut and `LLMClient` protocol\n",
    "- tqdm progress bar per step\n",
    "- Two-layer retry: API errors (429/500) with backoff + parse errors fed back to the LLM\n",
    "\n",
    "**Phase 3 — Field spec & dynamic prompts:**\n",
    "- 7-key field spec: `prompt`, `type`, `format`, `enum`, `examples`, `bad_examples`, `default`\n",
    "- Dynamic prompt builder (markdown headers + XML data boundaries)\n",
    "- Default enforcement — refusals replaced with field `default` in Python\n",
    "- FieldSpec validation — unknown keys rejected at construction time\n",
    "\n",
    "**Phase 4 — Caching & `list[dict]` input:**\n",
    "- SQLite-backed input-hash cache (`enable_caching=True`) — skip redundant API calls\n",
    "- Per-step cache stats: `cache_hits`, `cache_misses`, `cache_hit_rate`\n",
    "- `pipeline.clear_cache()` for full or per-step invalidation\n",
    "- `list[dict]` input: `pipeline.run([{...}])` returns `list[dict]` (output matches input type)\n",
    "- `FunctionStep(..., cache=False)` to disable caching for non-deterministic steps\n",
    "\n",
    "> **Note:** Jupyter runs its own event loop, so we use `await pipeline.run_async(df)`.\n",
    "> In scripts, use `pipeline.run(df)` (sync wrapper) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:21.117605Z",
     "iopub.status.busy": "2026-02-20T12:19:21.117405Z",
     "iopub.status.idle": "2026-02-20T12:19:23.172105Z",
     "shell.execute_reply": "2026-02-20T12:19:23.171832Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from accrue import Pipeline, LLMStep, FunctionStep, EnrichmentConfig, FieldSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1-header",
   "metadata": {},
   "source": [
    "## 1. Pipeline.run() → PipelineResult (Phase 2)\n",
    "\n",
    "`Pipeline.run(df)` is the primary API. It returns a `PipelineResult` with:\n",
    "- `.data` — enriched DataFrame\n",
    "- `.cost` — `CostSummary` with per-step token usage\n",
    "- `.errors` — list of `RowError` objects\n",
    "- `.success_rate` — fraction of rows that succeeded\n",
    "- `.has_errors` — quick boolean check\n",
    "\n",
    "The simplest case: 3 rows, 2 fields, one LLM call per row. Default model is `gpt-4.1-mini`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:23.173658Z",
     "iopub.status.busy": "2026-02-20T12:19:23.173535Z",
     "iopub.status.idle": "2026-02-20T12:19:23.183503Z",
     "shell.execute_reply": "2026-02-20T12:19:23.183239Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"company\": [\"Stripe\", \"Notion\", \"Figma\"],\n",
    "    \"description\": [\n",
    "        \"Online payment processing for internet businesses\",\n",
    "        \"All-in-one workspace for notes, docs, and project management\",\n",
    "        \"Collaborative interface design tool for teams\",\n",
    "    ],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-run",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:23.184735Z",
     "iopub.status.busy": "2026-02-20T12:19:23.184656Z",
     "iopub.status.idle": "2026-02-20T12:19:25.054504Z",
     "shell.execute_reply": "2026-02-20T12:19:25.054239Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    LLMStep(\"analyze\", fields={\n",
    "        \"category\": \"Classify into one of: Fintech, Productivity, Design, Other\",\n",
    "        \"target_market\": \"Describe the primary target market in 10 words or less\",\n",
    "    })\n",
    "])\n",
    "\n",
    "result = await pipeline.run_async(df)\n",
    "result.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1-cost",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.055911Z",
     "iopub.status.busy": "2026-02-20T12:19:25.055789Z",
     "iopub.status.idle": "2026-02-20T12:19:25.057985Z",
     "shell.execute_reply": "2026-02-20T12:19:25.057760Z"
    }
   },
   "outputs": [],
   "source": [
    "# PipelineResult gives you everything in one object\n",
    "print(f\"Success rate: {result.success_rate:.0%}\")\n",
    "print(f\"Has errors:   {result.has_errors}\")\n",
    "print(f\"Total tokens: {result.cost.total_tokens}\")\n",
    "print(f\"  Prompt:     {result.cost.total_prompt_tokens}\")\n",
    "print(f\"  Completion: {result.cost.total_completion_tokens}\")\n",
    "\n",
    "print(f\"\\nPer-step breakdown:\")\n",
    "for step_name, usage in result.cost.steps.items():\n",
    "    print(f\"  {step_name}: {usage.total_tokens} tokens, {usage.rows_processed} rows, model={usage.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2-header",
   "metadata": {},
   "source": [
    "## 2. Per-row error handling (Phase 2)\n",
    "\n",
    "With `on_error=\"continue\"` (the default), failed rows don't crash the pipeline.\n",
    "They produce `RowError` objects with sentinel `None` values, and the rest of the rows succeed normally.\n",
    "\n",
    "With `on_error=\"raise\"`, the pipeline fails fast on the first error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-error-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.059509Z",
     "iopub.status.busy": "2026-02-20T12:19:25.059392Z",
     "iopub.status.idle": "2026-02-20T12:19:25.065357Z",
     "shell.execute_reply": "2026-02-20T12:19:25.065112Z"
    }
   },
   "outputs": [],
   "source": [
    "def flaky_lookup(ctx):\n",
    "    \"\"\"Simulates an API that fails for unknown companies.\"\"\"\n",
    "    company = ctx.row[\"company\"]\n",
    "    if company == \"Notion\":\n",
    "        raise ConnectionError(f\"API timeout for {company}\")\n",
    "    return {\"status\": f\"{company} found\"}\n",
    "\n",
    "\n",
    "pipeline_err = Pipeline([\n",
    "    FunctionStep(\"lookup\", fn=flaky_lookup, fields=[\"status\"]),\n",
    "])\n",
    "\n",
    "config = EnrichmentConfig(enable_progress_bar=False, on_error=\"continue\")\n",
    "result = await pipeline_err.run_async(df, config)\n",
    "\n",
    "print(f\"Success rate: {result.success_rate:.0%}\")\n",
    "print(f\"Errors: {len(result.errors)}\\n\")\n",
    "\n",
    "for err in result.errors:\n",
    "    print(f\"  Row {err.row_index} ({df.iloc[err.row_index]['company']}): \"\n",
    "          f\"{err.error_type} \\u2014 {err.error}\")\n",
    "\n",
    "print(f\"\\nData (failed rows get None sentinels):\")\n",
    "result.data[[\"company\", \"status\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2-raise-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.066455Z",
     "iopub.status.busy": "2026-02-20T12:19:25.066383Z",
     "iopub.status.idle": "2026-02-20T12:19:25.069371Z",
     "shell.execute_reply": "2026-02-20T12:19:25.069143Z"
    }
   },
   "outputs": [],
   "source": [
    "# on_error=\"raise\" fails fast on the first error\n",
    "from accrue.core.exceptions import RowError\n",
    "\n",
    "config_raise = EnrichmentConfig(enable_progress_bar=False, on_error=\"raise\")\n",
    "try:\n",
    "    await pipeline_err.run_async(df, config_raise)\n",
    "except ConnectionError as e:\n",
    "    print(f\"Pipeline stopped immediately: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3-header",
   "metadata": {},
   "source": [
    "## 3. EnrichmentConfig presets (Phase 2)\n",
    "\n",
    "Three built-in presets cover common scenarios. Each tunes concurrency, logging, caching, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3-presets",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.070711Z",
     "iopub.status.busy": "2026-02-20T12:19:25.070634Z",
     "iopub.status.idle": "2026-02-20T12:19:25.073399Z",
     "shell.execute_reply": "2026-02-20T12:19:25.073131Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "for name, preset in [\n",
    "    (\"for_development()\", EnrichmentConfig.for_development()),\n",
    "    (\"for_production()\",  EnrichmentConfig.for_production()),\n",
    "    (\"for_server()\",      EnrichmentConfig.for_server()),\n",
    "]:\n",
    "    d = asdict(preset)\n",
    "    print(f\"{name}:\")\n",
    "    # Show only the fields that differ from defaults\n",
    "    defaults = asdict(EnrichmentConfig())\n",
    "    diff = {k: v for k, v in d.items() if v != defaults[k]}\n",
    "    for k, v in diff.items():\n",
    "        print(f\"  {k} = {v!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4-header",
   "metadata": {},
   "source": [
    "## 4. Provider flexibility (Phase 2)\n",
    "\n",
    "LLMStep uses the `LLMClient` protocol. OpenAI is the default (zero config).\n",
    "The `base_url` shortcut works with any OpenAI-compatible provider.\n",
    "Anthropic and Google ship as optional extras.\n",
    "\n",
    "```python\n",
    "# OpenAI-compatible providers (Ollama, Groq, DeepSeek, etc.)\n",
    "LLMStep(\"analyze\", fields={...}, model=\"llama3\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Anthropic: pip install accrue[anthropic]\n",
    "from accrue.providers import AnthropicClient\n",
    "LLMStep(\"analyze\", fields={...}, model=\"claude-sonnet-4-5-20250929\", client=AnthropicClient())\n",
    "\n",
    "# Google: pip install accrue[google]\n",
    "from accrue.providers import GoogleClient\n",
    "LLMStep(\"analyze\", fields={...}, model=\"gemini-2.5-flash\", client=GoogleClient())\n",
    "\n",
    "# Any provider — implement the ~30-line LLMClient protocol\n",
    "LLMStep(\"analyze\", fields={...}, client=MyCustomClient())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5-header",
   "metadata": {},
   "source": [
    "## 5. Multi-step pipeline with cost tracking (Phase 2)\n",
    "\n",
    "FunctionStep generates context, LLMStep uses it via `depends_on`.\n",
    "Internal `__` fields are filtered from output. Cost tracks across all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s5-multi-step",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.074735Z",
     "iopub.status.busy": "2026-02-20T12:19:25.074655Z",
     "iopub.status.idle": "2026-02-20T12:19:25.084699Z",
     "shell.execute_reply": "2026-02-20T12:19:25.084255Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_context(ctx):\n",
    "    \"\"\"Simulate an API lookup.\"\"\"\n",
    "    fake_data = {\n",
    "        \"Stripe\": \"Founded 2010. $95B valuation. 8000+ employees. Competes with Adyen, Square.\",\n",
    "        \"Notion\": \"Founded 2013. $10B valuation. 500+ employees. Competes with Confluence, Coda.\",\n",
    "        \"Figma\": \"Founded 2012. Acquired by Adobe for $20B (cancelled). Competes with Sketch, Canva.\",\n",
    "    }\n",
    "    company = ctx.row[\"company\"]\n",
    "    return {\"__context\": fake_data.get(company, \"No data available\")}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    FunctionStep(\"lookup\", fn=generate_context, fields=[\"__context\"]),\n",
    "    LLMStep(\"synthesize\", fields={\n",
    "        \"competitive_position\": {\n",
    "            \"prompt\": \"Rate the company's competitive position based on the context\",\n",
    "            \"enum\": [\"Leader\", \"Challenger\", \"Niche\"],\n",
    "        },\n",
    "        \"investment_thesis\": \"One-sentence investment thesis using context and description\",\n",
    "    }, depends_on=[\"lookup\"]),\n",
    "])\n",
    "\n",
    "result = await pipeline.run_async(df)\n",
    "\n",
    "print(\"Columns:\", list(result.data.columns))\n",
    "assert \"__context\" not in result.data.columns  # internal fields filtered\n",
    "\n",
    "print(f\"\\nCost across {len(result.cost.steps)} steps:\")\n",
    "for step_name, usage in result.cost.steps.items():\n",
    "    print(f\"  {step_name}: {usage.total_tokens} tokens, {usage.rows_processed} rows\")\n",
    "\n",
    "result.data[[\"company\", \"competitive_position\", \"investment_thesis\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Full 7-key field spec (Phase 3)\n",
    "\n",
    "Each field can use up to 7 keys:\n",
    "- `prompt` (required) — the extraction instruction\n",
    "- `type` — String, Number, Boolean, Date, List[String], JSON\n",
    "- `format` — output format pattern\n",
    "- `enum` — constrained value list (LLM MUST pick one)\n",
    "- `examples` — good output examples\n",
    "- `bad_examples` — anti-patterns to avoid\n",
    "- `default` — fallback when data is insufficient (enforced in Python, not by the LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-field-spec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.086062Z",
     "iopub.status.busy": "2026-02-20T12:19:25.085964Z",
     "iopub.status.idle": "2026-02-20T12:19:25.093599Z",
     "shell.execute_reply": "2026-02-20T12:19:25.093326Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    LLMStep(\"enrich\", fields={\n",
    "        \"sector\": {\n",
    "            \"prompt\": \"Classify the company's primary sector\",\n",
    "            \"enum\": [\"Fintech\", \"Productivity\", \"Design\", \"Infrastructure\", \"Other\"],\n",
    "            \"examples\": [\"Fintech\", \"Productivity\"],\n",
    "            \"bad_examples\": [\"Tech company\", \"Software\"],\n",
    "            \"default\": \"Other\",\n",
    "        },\n",
    "        \"employee_count\": {\n",
    "            \"prompt\": \"Estimate the number of employees\",\n",
    "            \"type\": \"Number\",\n",
    "            \"format\": \"integer\",\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"growth_stage\": {\n",
    "            \"prompt\": \"Classify the company's growth stage based on its description and market position\",\n",
    "            \"enum\": [\"Seed\", \"Growth\", \"Mature\", \"Decline\"],\n",
    "            \"examples\": [\"Growth - rapidly expanding market share\"],\n",
    "            \"default\": \"Unknown\",\n",
    "        },\n",
    "    })\n",
    "])\n",
    "\n",
    "result = await pipeline.run_async(df)\n",
    "result.data[[\"company\", \"sector\", \"employee_count\", \"growth_stage\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6-prompt-inspect",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.094883Z",
     "iopub.status.busy": "2026-02-20T12:19:25.094806Z",
     "iopub.status.idle": "2026-02-20T12:19:25.096900Z",
     "shell.execute_reply": "2026-02-20T12:19:25.096624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the generated system prompt to see the dynamic builder in action\n",
    "from accrue.steps.prompt_builder import build_system_message\n",
    "\n",
    "step = pipeline.get_step(\"enrich\")\n",
    "sample_prompt = build_system_message(\n",
    "    field_specs=step._field_specs,\n",
    "    row={\"company\": \"Stripe\", \"description\": \"Online payment processing\"},\n",
    ")\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7-header",
   "metadata": {},
   "source": [
    "## 7. Default enforcement (Phase 3)\n",
    "\n",
    "When the LLM can't determine a value, it returns refusal text like \"Unable to determine\".\n",
    "Default enforcement catches this in Python and replaces it with the field's `default`.\n",
    "\n",
    "We use a made-up company with no description to trigger refusals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7-defaults",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.098236Z",
     "iopub.status.busy": "2026-02-20T12:19:25.098152Z",
     "iopub.status.idle": "2026-02-20T12:19:25.104561Z",
     "shell.execute_reply": "2026-02-20T12:19:25.104344Z"
    }
   },
   "outputs": [],
   "source": [
    "df_unknown = pd.DataFrame({\n",
    "    \"company\": [\"Stripe\", \"Xylophonica Dynamics Ltd\"],\n",
    "    \"description\": [\n",
    "        \"Online payment processing for internet businesses\",\n",
    "        \"\",\n",
    "    ],\n",
    "})\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    LLMStep(\"analyze\", fields={\n",
    "        \"sector\": {\n",
    "            \"prompt\": \"What sector does this company operate in?\",\n",
    "            \"enum\": [\"Fintech\", \"SaaS\", \"Hardware\", \"Other\"],\n",
    "            \"default\": \"Unknown\",\n",
    "        },\n",
    "        \"founded_year\": {\n",
    "            \"prompt\": \"What year was this company founded?\",\n",
    "            \"type\": \"String\",\n",
    "            \"format\": \"YYYY\",\n",
    "            \"default\": \"N/A\",\n",
    "        },\n",
    "    })\n",
    "])\n",
    "\n",
    "result = await pipeline.run_async(df_unknown)\n",
    "result.data[[\"company\", \"sector\", \"founded_year\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s8-header",
   "metadata": {},
   "source": [
    "## 8. FieldSpec validation (Phase 3)\n",
    "\n",
    "Unknown keys are rejected at LLMStep construction time (not at runtime).\n",
    "This catches typos and legacy keys like `instructions` immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s8-validation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.106211Z",
     "iopub.status.busy": "2026-02-20T12:19:25.106113Z",
     "iopub.status.idle": "2026-02-20T12:19:25.108250Z",
     "shell.execute_reply": "2026-02-20T12:19:25.108015Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# This SHOULD fail — \"instructions\" is not a valid key (use \"prompt\" instead)\n",
    "try:\n",
    "    LLMStep(\"bad\", fields={\n",
    "        \"f1\": {\"prompt\": \"test\", \"instructions\": \"extra guidance\"},\n",
    "    })\n",
    "except ValidationError as e:\n",
    "    print(\"Caught validation error (as expected):\")\n",
    "    print(e.errors()[0][\"type\"], \"\\u2014\", e.errors()[0][\"msg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lawa735btzc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. SQLite-backed caching (Phase 4)\n",
    "\n",
    "With `enable_caching=True`, Accrue stores step results in a local SQLite database (`.accrue/cache.db`).\n",
    "The cache key is a SHA-256 hash of the step's full input: row data, prior results, field specs, model, and temperature.\n",
    "Changing any of these auto-invalidates the cache — no manual flushing needed.\n",
    "\n",
    "On the **first run**, all rows are cache misses (the step executes normally).\n",
    "On the **second run** with the same inputs, all rows are cache hits (zero API calls).\n",
    "\n",
    "Cache stats appear on `StepUsage`: `cache_hits`, `cache_misses`, and `cache_hit_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dqygcze06o4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.109711Z",
     "iopub.status.busy": "2026-02-20T12:19:25.109611Z",
     "iopub.status.idle": "2026-02-20T12:19:25.129180Z",
     "shell.execute_reply": "2026-02-20T12:19:25.128953Z"
    }
   },
   "outputs": [],
   "source": [
    "import tempfile, os\n",
    "\n",
    "# Use a temp directory so the demo doesn't pollute the working directory\n",
    "cache_dir = tempfile.mkdtemp()\n",
    "\n",
    "call_count = 0\n",
    "\n",
    "def counted_lookup(ctx):\n",
    "    \"\"\"A FunctionStep that counts how many times it's actually called.\"\"\"\n",
    "    global call_count\n",
    "    call_count += 1\n",
    "    company = ctx.row[\"company\"]\n",
    "    return {\"info\": f\"{company}: looked up\"}\n",
    "\n",
    "\n",
    "pipeline_cached = Pipeline([\n",
    "    FunctionStep(\"lookup\", fn=counted_lookup, fields=[\"info\"]),\n",
    "])\n",
    "\n",
    "config_cache = EnrichmentConfig(\n",
    "    enable_caching=True,\n",
    "    cache_dir=cache_dir,\n",
    "    enable_progress_bar=False,\n",
    ")\n",
    "\n",
    "# --- First run: all cache misses ---\n",
    "call_count = 0\n",
    "result1 = await pipeline_cached.run_async(df, config_cache)\n",
    "\n",
    "usage1 = result1.cost.steps[\"lookup\"]\n",
    "print(\"=== First run ===\")\n",
    "print(f\"  Function called:  {call_count} times\")\n",
    "print(f\"  cache_hits:       {usage1.cache_hits}\")\n",
    "print(f\"  cache_misses:     {usage1.cache_misses}\")\n",
    "print(f\"  cache_hit_rate:   {usage1.cache_hit_rate:.0%}\")\n",
    "\n",
    "# --- Second run: all cache hits (zero function calls) ---\n",
    "call_count = 0\n",
    "result2 = await pipeline_cached.run_async(df, config_cache)\n",
    "\n",
    "usage2 = result2.cost.steps[\"lookup\"]\n",
    "print(\"\\n=== Second run (cached) ===\")\n",
    "print(f\"  Function called:  {call_count} times\")\n",
    "print(f\"  cache_hits:       {usage2.cache_hits}\")\n",
    "print(f\"  cache_misses:     {usage2.cache_misses}\")\n",
    "print(f\"  cache_hit_rate:   {usage2.cache_hit_rate:.0%}\")\n",
    "\n",
    "# Data is identical\n",
    "assert result1.data.equals(result2.data)\n",
    "print(\"\\nData matches between runs ✓\")\n",
    "result2.data[[\"company\", \"info\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g6cnpf8j1va",
   "metadata": {},
   "source": [
    "## 10. Cache invalidation and `clear_cache()` (Phase 4)\n",
    "\n",
    "`pipeline.clear_cache()` wipes all cached entries. `pipeline.clear_cache(step=\"name\")` wipes only one step.\n",
    "\n",
    "Caching also auto-invalidates when inputs change — if you modify a row's data or change a field spec,\n",
    "the cache key changes and the step re-executes for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "siomz0918fm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.130396Z",
     "iopub.status.busy": "2026-02-20T12:19:25.130324Z",
     "iopub.status.idle": "2026-02-20T12:19:25.149254Z",
     "shell.execute_reply": "2026-02-20T12:19:25.149024Z"
    }
   },
   "outputs": [],
   "source": [
    "# clear_cache() returns the number of entries deleted\n",
    "deleted = pipeline_cached.clear_cache(cache_dir=cache_dir)\n",
    "print(f\"Cleared {deleted} cache entries\")\n",
    "\n",
    "# After clearing, the next run is all cache misses again\n",
    "call_count = 0\n",
    "result3 = await pipeline_cached.run_async(df, config_cache)\n",
    "usage3 = result3.cost.steps[\"lookup\"]\n",
    "print(f\"\\nAfter clear — cache_misses: {usage3.cache_misses}, function called: {call_count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "idsp3fnfcfh",
   "metadata": {},
   "source": [
    "## 11. `cache=False` per step (Phase 4)\n",
    "\n",
    "Non-deterministic FunctionSteps (e.g. current time, random sampling) should skip caching.\n",
    "Set `cache=False` on the step. FunctionSteps can also use `cache_version=\"v1\"` — bumping\n",
    "the version string invalidates all cached entries for that step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uq5v0jt9csm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.150504Z",
     "iopub.status.busy": "2026-02-20T12:19:25.150433Z",
     "iopub.status.idle": "2026-02-20T12:19:25.154673Z",
     "shell.execute_reply": "2026-02-20T12:19:25.154442Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_score(ctx):\n",
    "    \"\"\"Non-deterministic — should NOT be cached.\"\"\"\n",
    "    return {\"score\": random.randint(1, 100)}\n",
    "\n",
    "\n",
    "pipeline_nocache = Pipeline([\n",
    "    FunctionStep(\"rand\", fn=random_score, fields=[\"score\"], cache=False),\n",
    "])\n",
    "\n",
    "# Even with caching enabled globally, this step always re-executes\n",
    "r1 = await pipeline_nocache.run_async(df, config_cache)\n",
    "r2 = await pipeline_nocache.run_async(df, config_cache)\n",
    "\n",
    "print(\"Run 1 scores:\", list(r1.data[\"score\"]))\n",
    "print(\"Run 2 scores:\", list(r2.data[\"score\"]))\n",
    "print(\"Different?\", not r1.data[\"score\"].equals(r2.data[\"score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3sxqzkcad1h",
   "metadata": {},
   "source": [
    "## 12. `list[dict]` input (Phase 4)\n",
    "\n",
    "`Pipeline.run()` also accepts `list[dict]` — useful for server contexts, test code,\n",
    "or Polars users (`polars_df.to_dicts()`). The output type matches the input type:\n",
    "DataFrame in → DataFrame out, `list[dict]` in → `list[dict]` out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dungl4i7py",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.155875Z",
     "iopub.status.busy": "2026-02-20T12:19:25.155802Z",
     "iopub.status.idle": "2026-02-20T12:19:25.158590Z",
     "shell.execute_reply": "2026-02-20T12:19:25.158237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass list[dict] instead of DataFrame\n",
    "rows = [\n",
    "    {\"company\": \"Stripe\", \"description\": \"Online payment processing\"},\n",
    "    {\"company\": \"Notion\", \"description\": \"All-in-one workspace\"},\n",
    "]\n",
    "\n",
    "pipeline_simple = Pipeline([\n",
    "    FunctionStep(\"tag\", fn=lambda ctx: {\"tag\": ctx.row[\"company\"].lower()}, fields=[\"tag\"]),\n",
    "])\n",
    "\n",
    "config_quiet = EnrichmentConfig(enable_progress_bar=False)\n",
    "result = await pipeline_simple.run_async(rows, config_quiet)\n",
    "\n",
    "# Output is list[dict], not DataFrame\n",
    "print(f\"Input type:  {type(rows).__name__}\")\n",
    "print(f\"Output type: {type(result.data).__name__}\")\n",
    "print(f\"Success rate: {result.success_rate:.0%}\\n\")\n",
    "\n",
    "for row in result.data:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10nnecvgdh7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T12:19:25.159836Z",
     "iopub.status.busy": "2026-02-20T12:19:25.159759Z",
     "iopub.status.idle": "2026-02-20T12:19:25.162427Z",
     "shell.execute_reply": "2026-02-20T12:19:25.162158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup temp cache directory\n",
    "import shutil\n",
    "shutil.rmtree(cache_dir, ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
